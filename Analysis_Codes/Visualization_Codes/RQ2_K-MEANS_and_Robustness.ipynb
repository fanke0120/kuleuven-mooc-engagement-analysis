{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyPtZbrGHPlMWMl84r6V/Is7"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","source":["# ==============================================================================\n","# Python Scripts for K-Means Clustering Pipeline\n","#\n","# This file contains the complete data processing and analysis pipeline for the\n","# K-Means clustering analysis section of the thesis. It is organized into\n","# several sequential scripts:\n","#\n","# Script 1-4: Feature Engineering from raw data files.\n","# Script 5: Merging all features into a final matrix.\n","# Script 6: Performing K-Means clustering and robustness checks.\n","# Script 7-8: Validating the resulting clusters against course outcomes.\n","# Script 9: Standalone script for K-Means robustness check (50 runs).\n","#\n","# Author: Ziyun Ke\n","# Date: August 2025\n","# ==============================================================================\n","\n","import pandas as pd\n","import numpy as np\n","import matplotlib.pyplot as plt\n","import seaborn as sns\n","from sklearn.preprocessing import StandardScaler\n","from sklearn.cluster import KMeans\n","from sklearn.metrics import silhouette_score\n","from sklearn.decomposition import PCA\n","\n","# ==============================================================================\n","# SCRIPT 1: FEATURE ENGINEERING - ASSESSMENT ENGAGEMENT\n","# ==============================================================================\n","print(\"--- Running Script 1: Feature Engineering - Assessment Engagement ---\")\n","\n","# --- 0. Configuration ---\n","# NOTE: This script needs to be run for each course by changing the input file.\n","input_file_assess = 'assessments_course-v1_KULeuvenX+EUROHISx+1T2023.csv'\n","output_file_assess = 'feature_assessment_performance.csv'\n","\n","# Please verify these column names match your file\n","USER_ID_COL_ASSESS = 'course_learner_id'\n","GRADE_COL = 'grade'\n","MAX_GRADE_COL = 'max_grade'\n","# --- End of Configuration ---\n","\n","try:\n","    # --- 1. Load and Preprocess Data ---\n","    print(f\"--- Loading assessment data: {input_file_assess} ---\")\n","    df_assess = pd.read_csv(input_file_assess)\n","    print(f\"File loaded successfully! Found {len(df_assess)} assessment records.\")\n","\n","    required_cols_assess = [USER_ID_COL_ASSESS, GRADE_COL, MAX_GRADE_COL]\n","    if not all(col in df_assess.columns for col in required_cols_assess):\n","        raise KeyError(f\"File is missing required columns. Needed {required_cols_assess}, but found {list(df_assess.columns)}\")\n","\n","    # Clean data: ensure grades are numeric and handle max_grade being zero\n","    df_assess[GRADE_COL] = pd.to_numeric(df_assess[GRADE_COL], errors='coerce')\n","    df_assess[MAX_GRADE_COL] = pd.to_numeric(df_assess[MAX_GRADE_COL], errors='coerce').replace(0, np.nan)\n","    df_assess.dropna(subset=[USER_ID_COL_ASSESS, GRADE_COL, MAX_GRADE_COL], inplace=True)\n","    print(\"Data cleaning complete.\")\n","\n","    # --- 2. Calculate Performance and Participation Features ---\n","    print(\"Grouping by student ID and calculating mean performance and participation count...\")\n","    df_assess['score_percentage'] = df_assess[GRADE_COL] / df_assess[MAX_GRADE_COL]\n","\n","    # Core step: Use .agg() to calculate two metrics at once\n","    feature_df_assess = df_assess.groupby(USER_ID_COL_ASSESS).agg(\n","        Overall_Performance=('score_percentage', 'mean'),\n","        Assessment_Participation_Count=(GRADE_COL, 'count')\n","    )\n","    feature_df_assess['Overall_Performance'] = (feature_df_assess['Overall_Performance'] * 100).round(2)\n","    print(\"Feature calculation complete!\")\n","\n","    # --- 3. Format and Save Results ---\n","    feature_df_assess = feature_df_assess.reset_index()\n","    print(\"\\n--- Student Performance & Participation Features (First 10 rows) ---\")\n","    print(feature_df_assess.head(10))\n","    feature_df_assess.to_csv(output_file_assess, index=False)\n","    print(f\"\\nSuccess! Results saved to '{output_file_assess}'.\")\n","\n","except FileNotFoundError:\n","    print(f\"ERROR: File not found '{input_file_assess}'.\")\n","except KeyError as e:\n","    print(f\"ERROR: {e}\")\n","except Exception as e:\n","    print(f\"An unexpected error occurred: {e}\")\n","\n","# ==============================================================================\n","# SCRIPT 2: FEATURE ENGINEERING - VIDEO ENGAGEMENT\n","# ==============================================================================\n","print(\"\\n\\n--- Running Script 2: Feature Engineering - Video Engagement ---\")\n","\n","# --- 0. Configuration ---\n","input_file_video = 'video_interactions_course-v1_KULeuvenX+EUROHISx+1T2023.csv'\n","output_file_video = 'feature_video_engagement.csv'\n","USER_ID_COL_VIDEO = 'course_learner_id'\n","DURATION_COL = 'duration'\n","PAUSE_COL = 'times_pause'\n","# --- End of Configuration ---\n","\n","try:\n","    # --- 1. Load and Clean Data ---\n","    print(f\"--- Loading data file: {input_file_video} ---\")\n","    df_video = pd.read_csv(input_file_video)\n","    print(f\"File loaded successfully! Found {len(df_video)} interaction records.\")\n","    df_video[DURATION_COL] = pd.to_numeric(df_video[DURATION_COL], errors='coerce')\n","    df_video[PAUSE_COL] = pd.to_numeric(df_video[PAUSE_COL], errors='coerce')\n","    df_video.dropna(subset=[USER_ID_COL_VIDEO, DURATION_COL, PAUSE_COL], inplace=True)\n","    print(\"Data cleaning complete.\")\n","\n","    # --- 2. Calculate Features ---\n","    print(f\"Grouping by '{USER_ID_COL_VIDEO}' and calculating total duration and pauses...\")\n","    video_features_df = df_video.groupby(USER_ID_COL_VIDEO).agg(\n","        Total_Video_Duration_Seconds=(DURATION_COL, 'sum'),\n","        Total_Pause_Count=(PAUSE_COL, 'sum')\n","    )\n","    video_features_df['Total_Pause_Count'] = video_features_df['Total_Pause_Count'].astype(int)\n","    print(\"Calculation complete!\")\n","\n","    # --- 3. Format and Save Results ---\n","    video_features_df = video_features_df.reset_index()\n","    print(\"\\n--- Video Interaction Features per User (First 10 rows) ---\")\n","    print(video_features_df.head(10))\n","    video_features_df.to_csv(output_file_video, index=False)\n","    print(f\"\\nSuccess! Results saved to '{output_file_video}'.\")\n","\n","except FileNotFoundError:\n","    print(f\"ERROR: File not found '{input_file_video}'.\")\n","except KeyError as e:\n","    print(f\"ERROR: Column not found {e}.\")\n","\n","# ==============================================================================\n","# SCRIPT 3: FEATURE ENGINEERING - FORUM ENGAGEMENT\n","# ==============================================================================\n","print(\"\\n\\n--- Running Script 3: Feature Engineering - Forum Engagement ---\")\n","\n","# --- 0. Configuration ---\n","input_file_forum = 'forum_interaction_course-v1_KULeuvenX+EUROHISx+1T2023.csv'\n","output_file_forum = 'feature_forum_engagement.csv'\n","USER_ID_COL_FORUM = 'course_learner_id'\n","BODY_COL = 'post_content'\n","# --- End of Configuration ---\n","\n","try:\n","    # --- 1. Load and Preprocess Data ---\n","    print(f\"--- Loading forum data: {input_file_forum} ---\")\n","    df_forum = pd.read_csv(input_file_forum)\n","    print(f\"File loaded successfully! Found {len(df_forum)} forum records.\")\n","    df_forum[BODY_COL].fillna('', inplace=True)\n","    df_forum['post_content_length'] = df_forum[BODY_COL].str.len()\n","    df_forum.dropna(subset=[USER_ID_COL_FORUM], inplace=True)\n","    print(\"Data preprocessing complete.\")\n","\n","    # --- 2. Calculate Forum Engagement Features ---\n","    print(\"Grouping by student ID and calculating total post count and average post length...\")\n","    feature_df_forum = df_forum.groupby(USER_ID_COL_FORUM).agg(\n","        Total_Post_Count=(BODY_COL, 'count'),\n","        Average_Post_Length=('post_content_length', 'mean')\n","    )\n","    feature_df_forum['Average_Post_Length'] = feature_df_forum['Average_Post_Length'].round(1)\n","    print(\"Feature calculation complete!\")\n","\n","    # --- 3. Format and Save Results ---\n","    feature_df_forum = feature_df_forum.reset_index()\n","    print(\"\\n--- Forum Engagement Features per User (First 10 rows) ---\")\n","    print(feature_df_forum.head(10))\n","    feature_df_forum.to_csv(output_file_forum, index=False)\n","    print(f\"\\nSuccess! Results saved to '{output_file_forum}'.\")\n","\n","except FileNotFoundError:\n","    print(f\"ERROR: File not found '{input_file_forum}'.\")\n","except KeyError as e:\n","    print(f\"ERROR: Column not found {e}.\")\n","\n","# ==============================================================================\n","# SCRIPT 4: FEATURE ENGINEERING - TEMPORAL PATTERNS (ACTIVE WEEKS)\n","# ==============================================================================\n","print(\"\\n\\n--- Running Script 4: Feature Engineering - Active Weeks ---\")\n","\n","# --- 0. Configuration ---\n","input_file_sessions = 'sessions_course-v1_KULeuvenX+EUROHISx+1T2023.csv'\n","output_file_weeks = 'feature_active_weeks_per_user.csv'\n","USER_ID_COL_SESSIONS = 'course_learner_id'\n","TIMESTAMP_COL = 'start_time'\n","# --- End of Configuration ---\n","\n","try:\n","    # --- 1. Load and Preprocess Data ---\n","    print(f\"--- Loading session data: {input_file_sessions} ---\")\n","    df_sessions = pd.read_csv(input_file_sessions)\n","    print(f\"File loaded successfully! Found {len(df_sessions)} session records.\")\n","    print(\"Cleaning timestamp format...\")\n","    df_sessions[TIMESTAMP_COL] = df_sessions[TIMESTAMP_COL].str.replace(r' GMT\\+\\d{4} \\(.*\\)', '', regex=True)\n","    df_sessions[TIMESTAMP_COL] = pd.to_datetime(df_sessions[TIMESTAMP_COL], format='%a %b %d %Y %H:%M:%S', errors='coerce')\n","    df_sessions.dropna(subset=[USER_ID_COL_SESSIONS, TIMESTAMP_COL], inplace=True)\n","    print(\"Data preprocessing complete.\")\n","\n","    # --- 2. Calculate Active Weeks per User ---\n","    print(f\"Grouping by '{USER_ID_COL_SESSIONS}' and calculating unique weeks...\")\n","    df_sessions['activity_week'] = df_sessions[TIMESTAMP_COL].dt.to_period('W')\n","    active_weeks_per_user = df_sessions.groupby(USER_ID_COL_SESSIONS)['activity_week'].nunique()\n","\n","    # --- 3. Format and Save Results ---\n","    active_weeks_df = active_weeks_per_user.reset_index()\n","    active_weeks_df.columns = [USER_ID_COL_SESSIONS, 'Number_of_Active_Weeks']\n","    print(\"Calculation complete!\")\n","    print(\"\\n--- Active Weeks per User (First 10 rows) ---\")\n","    print(active_weeks_df.head(10))\n","    active_weeks_df.to_csv(output_file_weeks, index=False)\n","    print(f\"\\nSuccess! Results saved to '{output_file_weeks}'.\")\n","\n","except FileNotFoundError:\n","    print(f\"ERROR: File not found '{input_file_sessions}'.\")\n","except KeyError as e:\n","    print(f\"ERROR: Column not found {e}.\")\n","\n","# ==============================================================================\n","# SCRIPT 5: MERGE ALL FEATURES INTO FINAL MATRIX\n","# ==============================================================================\n","print(\"\\n\\n--- Running Script 5: Merging All Features ---\")\n","\n","# --- 0. Configuration ---\n","USER_ID_COL_MERGE = 'course_learner_id'\n","output_file_matrix = 'final_feature_matrix_for_clustering.csv'\n","# --- End of Configuration ---\n","\n","try:\n","    # --- 1. Load All Individual Feature Files ---\n","    print(\"--- Loading all feature files ---\")\n","    df_assess = pd.read_csv(output_file_assess)\n","    print(f\" -> Loaded assessment features for {len(df_assess)} students.\")\n","    df_forum = pd.read_csv(output_file_forum)\n","    print(f\" -> Loaded forum features for {len(df_forum)} students.\")\n","    df_video = pd.read_csv(output_file_video)\n","    print(f\" -> Loaded video features for {len(df_video)} students.\")\n","    df_weeks = pd.read_csv(output_file_weeks)\n","    print(f\" -> Loaded active weeks feature for {len(df_weeks)} students.\")\n","\n","    # --- 2. Chained Merge ---\n","    print(\"\\n--- Merging all features into a single matrix ---\")\n","    # Use 'outer' merge to keep all students who appear in at least one file\n","    merged_df = pd.merge(df_weeks, df_video, on=USER_ID_COL_MERGE, how='outer')\n","    merged_df = pd.merge(merged_df, df_forum, on=USER_ID_COL_MERGE, how='outer')\n","    final_feature_matrix = pd.merge(merged_df, df_assess, on=USER_ID_COL_MERGE, how='outer')\n","    print(\"Initial merge complete. Now handling missing values...\")\n","\n","    # --- 3. Fill Missing Values ---\n","    # Critical step: NaNs resulting from an outer merge mean \"no participation\" for that feature.\n","    # We replace these NaNs with 0.\n","    final_feature_matrix.fillna(0, inplace=True)\n","    print(\"Missing values filled with 0.\")\n","\n","    # --- 4. Display and Save Final Result ---\n","    print(\"\\n--- Final Feature Matrix Preview (First 10 rows) ---\")\n","    print(final_feature_matrix.head(10))\n","    print(f\"\\nFinal matrix contains {len(final_feature_matrix)} students and {len(final_feature_matrix.columns)} features.\")\n","    print(\"Columns:\", list(final_feature_matrix.columns))\n","    final_feature_matrix.to_csv(output_file_matrix, index=False)\n","    print(f\"\\nSuccess! Final feature matrix saved to '{output_file_matrix}'.\")\n","\n","except FileNotFoundError as e:\n","    print(f\"\\nERROR: File not found {e.filename}. Ensure all feature files were generated correctly.\")\n","except Exception as e:\n","    print(f\"An error occurred during processing: {e}\")\n","\n","# ==============================================================================\n","# SCRIPT 6: K-MEANS CLUSTERING\n","# ==============================================================================\n","print(\"\\n\\n--- Running Script 6: K-Means Clustering ---\")\n","\n","# --- 0. Configuration ---\n","input_file_cluster = 'final_feature_matrix_for_clustering.csv'\n","USER_ID_COL_CLUSTER = 'course_learner_id'\n","output_labels_file = 'student_cluster_labels.csv'\n","# --- End of Configuration ---\n","\n","try:\n","    # --- 1. Load and Prepare Data ---\n","    print(\"--- Step 1: Loading feature matrix ---\")\n","    df_cluster = pd.read_csv(input_file_cluster)\n","    user_ids = df_cluster[USER_ID_COL_CLUSTER]\n","    features = df_cluster.drop(columns=[USER_ID_COL_CLUSTER])\n","    print(f\"Data loaded successfully! Clustering {len(features.columns)} features for {len(features)} students.\")\n","\n","    # --- 2. Feature Standardization ---\n","    print(\"\\n--- Step 2: Standardizing features ---\")\n","    scaler = StandardScaler()\n","    features_scaled = scaler.fit_transform(features)\n","    print(\"Standardization complete.\")\n","\n","    # --- 3. Find Optimal K using Silhouette Score (Optional) ---\n","    print(\"\\n--- Step 3: Finding optimal K using Silhouette Score ---\")\n","    k_range = range(2, 11)\n","    silhouette_scores = []\n","    for k in k_range:\n","        kmeans = KMeans(n_clusters=k, init='k-means++', random_state=42, n_init=10)\n","        kmeans.fit(features_scaled)\n","        score = silhouette_score(features_scaled, kmeans.labels_)\n","        silhouette_scores.append(score)\n","    best_k = k_range[np.argmax(silhouette_scores)]\n","    print(f\"Diagnosis: Highest silhouette score found at K = {best_k}.\")\n","\n","    # Plot silhouette scores\n","    plt.figure(figsize=(10, 6))\n","    plt.plot(k_range, silhouette_scores, marker='o')\n","    plt.title('Silhouette Score for Optimal K')\n","    plt.xlabel('Number of Clusters (K)')\n","    plt.ylabel('Silhouette Score')\n","    plt.grid(True)\n","    plt.show()\n","\n","    # --- 4. Perform Final Clustering ---\n","    optimal_k = 2  # Set K=2 based on the analysis\n","    print(f\"\\n--- Step 4: Performing final clustering with K = {optimal_k} ---\")\n","    kmeans_final = KMeans(n_clusters=optimal_k, init='k-means++', random_state=42, n_init=10)\n","    final_labels = kmeans_final.fit_predict(features_scaled)\n","    df_cluster['Cluster'] = final_labels\n","    print(\"Clustering complete.\")\n","\n","    # --- 5. Interpret Cluster Centroids ---\n","    print(\"\\n--- Step 5: Interpreting cluster centroids ---\")\n","    centroids_original_scale = scaler.inverse_transform(kmeans_final.cluster_centers_)\n","    centroid_df = pd.DataFrame(centroids_original_scale, columns=features.columns)\n","    centroid_df.index.name = 'Cluster'\n","    print(centroid_df.round(1))\n","\n","    # --- 6. Visualize Clusters with PCA (Optional) ---\n","    print(\"\\n--- Step 6: Visualizing clusters with PCA ---\")\n","    pca = PCA(n_components=2)\n","    features_pca = pca.fit_transform(features_scaled)\n","    df_pca = pd.DataFrame(data=features_pca, columns=['PC1', 'PC2'])\n","    df_pca['Cluster'] = final_labels\n","    plt.figure(figsize=(12, 8))\n","    sns.scatterplot(x='PC1', y='PC2', hue='Cluster', palette=sns.color_palette(\"hls\", optimal_k), data=df_pca, legend=\"full\")\n","    plt.title('K-Means Clustering of Learners (PCA Visualization)')\n","    plt.show()\n","\n","    # --- 7. Generate and Save Final Student Labels ---\n","    print(\"\\n--- Step 7: Generating final student label file ---\")\n","    final_student_labels_df = df_cluster[[USER_ID_COL_CLUSTER, 'Cluster']]\n","    # Map cluster numbers to meaningful names based on centroid interpretation\n","    cluster_names = {0: 'More Active Learners', 1: 'Less Active Learners'} # Adjust based on your interpretation\n","    final_student_labels_df['Cluster_Name'] = final_student_labels_df['Cluster'].map(cluster_names)\n","    final_student_labels_df.to_csv(output_labels_file, index=False)\n","    print(f\"\\nSuccess! Student cluster labels saved to '{output_labels_file}'.\")\n","    print(\"File preview:\")\n","    print(final_student_labels_df.head())\n","\n","except FileNotFoundError:\n","    print(f\"ERROR: File not found '{input_file_cluster}'.\")\n","except Exception as e:\n","    print(f\"An error occurred during processing: {e}\")\n","\n","# ==============================================================================\n","# SCRIPT 7: FILTER FOR PASSING STUDENTS\n","# ==============================================================================\n","print(\"\\n\\n--- Running Script 7: Filtering for Passing Students ---\")\n","\n","# --- 0. Configuration ---\n","input_file_roster = 'course_learner_course-v1_KULeuvenX+EUROHISx+1T2023.csv'\n","output_file_passing = 'passing_students_only.csv'\n","CERTIFICATE_STATUS_COL = 'certificate_status'\n","# --- End of Configuration ---\n","\n","try:\n","    # --- 1. Load Data ---\n","    print(f\"--- Loading data file: {input_file_roster} ---\")\n","    df_roster = pd.read_csv(input_file_roster)\n","    print(f\"File loaded successfully! Found {len(df_roster)} records.\")\n","\n","    # --- 2. Filter Data ---\n","    if CERTIFICATE_STATUS_COL not in df_roster.columns:\n","        raise KeyError(f\"Column '{CERTIFICATE_STATUS_COL}' not found. Available columns: {list(df_roster.columns)}\")\n","    print(f\"\\n--- Filtering for records where '{CERTIFICATE_STATUS_COL}' is 'downloadable' ---\")\n","    df_passing = df_roster[df_roster[CERTIFICATE_STATUS_COL] == 'downloadable'].copy()\n","    print(\"Filtering complete.\")\n","    print(f\"Original records: {len(df_roster)}\")\n","    print(f\"Records with 'downloadable' certificate: {len(df_passing)}\")\n","\n","    # --- 3. Display and Save Results ---\n","    print(\"\\n--- Preview of passing students (First 5 rows) ---\")\n","    print(df_passing.head())\n","    df_passing.to_csv(output_file_passing, index=False)\n","    print(f\"\\nSuccess! Filtered data saved to '{output_file_passing}'.\")\n","\n","except FileNotFoundError:\n","    print(f\"ERROR: File not found '{input_file_roster}'.\")\n","except KeyError as e:\n","    print(f\"ERROR: {e}\")\n","except Exception as e:\n","    print(f\"An unexpected error occurred: {e}\")\n","\n","# ==============================================================================\n","# SCRIPT 8: VALIDATE CLUSTERS WITH PASSING STATUS\n","# ==============================================================================\n","print(\"\\n\\n--- Running Script 8: Validating Clusters ---\")\n","\n","# --- 0. Configuration ---\n","clusters_file = 'student_cluster_labels.csv'\n","passing_file = 'passing_students_only.csv'\n","USER_ID_COL_VALIDATE = 'course_learner_id'\n","# --- End of Configuration ---\n","\n","try:\n","    # --- 1. Load Data ---\n","    print(\"--- Step 1: Loading data files ---\")\n","    df_clusters = pd.read_csv(clusters_file)\n","    print(f\" -> Loaded cluster labels for {len(df_clusters)} students.\")\n","    df_passing = pd.read_csv(passing_file)\n","    print(f\" -> Loaded passing roster for {len(df_passing)} students.\")\n","\n","    # --- 2. Prepare and Merge Data ---\n","    print(\"\\n--- Step 2: Merging data and marking passing/not passing status ---\")\n","    df_passing['Status'] = 'Passing'\n","    # Use a left merge to keep all students from the clustering analysis\n","    merged_df_validate = pd.merge(df_clusters, df_passing[[USER_ID_COL_VALIDATE, 'Status']], on=USER_ID_COL_VALIDATE, how='left')\n","    # Fill NaNs with 'Not Passing' for students not in the passing list\n","    merged_df_validate['Status'].fillna('Not Passing', inplace=True)\n","    print(\"Data merge and status marking complete.\")\n","\n","    # --- 3. Create Contingency Table ---\n","    print(\"\\n--- Step 3: Generating contingency table ---\")\n","    contingency_table = pd.crosstab(\n","        index=merged_df_validate['Cluster_Name'],\n","        columns=merged_df_validate['Status']\n","    )\n","    print(\"Contingency table generated successfully!\")\n","    print(contingency_table)\n","\n","    # --- 4. Calculate Pass Rates ---\n","    print(\"\\n--- Step 4: Calculating pass rates for each cluster ---\")\n","    # Calculate pass rate for each cluster name found in the table\n","    for cluster_name in contingency_table.index:\n","        passing_count = contingency_table.loc[cluster_name, 'Passing']\n","        total_count = contingency_table.loc[cluster_name].sum()\n","        pass_rate = (passing_count / total_count) * 100\n","        print(f\"Pass rate for '{cluster_name}': {pass_rate:.1f}%\")\n","\n","except FileNotFoundError as e:\n","    print(f\"\\nERROR: File not found {e.filename}. Please ensure both CSV files are available.\")\n","except KeyError as e:\n","    print(f\"\\nERROR: Column not found {e}. This often means the user ID column names do not match.\")\n","except Exception as e:\n","    print(f\"An unexpected error occurred: {e}\")\n","\n","# ==============================================================================\n","# SCRIPT 9: K-MEANS ROBUSTNESS CHECK (50 RUNS)\n","# ==============================================================================\n","print(\"\\n\\n--- Running Script 9: K-Means Robustness Check ---\")\n","\n","# --- 0. Configuration ---\n","# NOTE: This script needs to be run for each course by changing the input file.\n","input_file_robustness = 'final_feature_matrix_for_clustering.csv'\n","USER_ID_COL_ROBUSTNESS = 'course_learner_id'\n","summary_output_file = 'cluster_robustness_summary.csv'\n","N_RUNS = 50\n","OPTIMAL_K_ROBUSTNESS = 2\n","# --- End of Configuration ---\n","\n","try:\n","    # --- 1. Load and Prepare Data ---\n","    print(\"--- Step 1: Loading feature matrix for robustness check ---\")\n","    df_robustness_data = pd.read_csv(input_file_robustness)\n","    features_robustness = df_robustness_data.drop(columns=[USER_ID_COL_ROBUSTNESS])\n","    print(f\"Data loaded successfully!\")\n","\n","    # --- 2. Feature Standardization ---\n","    print(\"\\n--- Step 2: Standardizing features ---\")\n","    scaler_robustness = StandardScaler()\n","    features_scaled_robustness = scaler_robustness.fit_transform(features_robustness)\n","    print(\"Standardization complete.\")\n","\n","    # --- 3. Run Clustering 50 Times ---\n","    cluster_size_results = []\n","    print(f\"Running K-Means {N_RUNS} times with K={OPTIMAL_K_ROBUSTNESS} to check for stability...\")\n","\n","    for i in range(N_RUNS):\n","        # Key: Use a different random_state for each iteration\n","        kmeans_robustness = KMeans(n_clusters=OPTIMAL_K_ROBUSTNESS, init='k-means++', random_state=i, n_init=10)\n","        labels = kmeans_robustness.fit_predict(features_scaled_robustness)\n","\n","        # Count the size of each cluster\n","        cluster_sizes = pd.Series(labels).value_counts().sort_index()\n","\n","        # Store the results in a list\n","        result_row = {'run': i + 1}\n","        for cluster_id in range(OPTIMAL_K_ROBUSTNESS):\n","            result_row[f'cluster_{cluster_id}_size'] = cluster_sizes.get(cluster_id, 0)\n","        cluster_size_results.append(result_row)\n","\n","    # --- 4. Analyze and Save Results ---\n","    df_robustness_results = pd.DataFrame(cluster_size_results)\n","    print(\"\\n--- Robustness Check Results ---\")\n","    print(f\"Completed {N_RUNS} clustering runs. Descriptive statistics for cluster sizes are as follows:\")\n","\n","    # Calculate and print descriptive statistics for the appendix\n","    cluster_size_summary = df_robustness_results.drop(columns=['run']).describe()\n","    print(cluster_size_summary.round(2))\n","\n","    # --- End of Robustness Check ---\n","\n","except FileNotFoundError:\n","    print(f\"ERROR: File not found '{input_file_robustness}'.\")\n","except Exception as e:\n","    print(f\"An error occurred during processing: {e}\")"],"metadata":{"id":"y3QxUGJDPLHe"},"execution_count":null,"outputs":[]}]}